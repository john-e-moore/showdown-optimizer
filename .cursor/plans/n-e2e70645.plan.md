<!-- e2e70645-586d-4cd2-be3f-d2d0d91a5eff 20e91d08-53d3-4ddd-bf84-423c38bed13c -->
# NFL Showdown Correlation Pipeline Implementation Plan

## High-level workflow

- **Goal**: Implement the first-stage pipeline that ingests historical nflverse data, computes DK fantasy points and per-player z-scores, builds a pairwise training dataset, trains a correlation regressor, and then generates a player correlation matrix for a single Sabersim Showdown CSV, writing an Excel output.
- **Execution entrypoint**: `src/main.py` as a simple CLI/script that orchestrates the end-to-end flow and writes the Excel file.

## 1. Project skeleton and configuration

- **Create directories**: `data/nfl_raw/`, `data/nfl_processed/`, `data/sabersim/`, `models/`, `outputs/correlations/` (ensure code creates directories on demand if missing).
- **Add config module** ([`src/config.py`](src/config.py)):
- Define paths: `NFL_PLAYER_GAMES_PARQUET`, `NFL_GAMES_PARQUET`, `SABERSIM_CSV`, `OUTPUT_CORR_EXCEL`, `MIN_PLAYER_GAMES` matching the prompt.
- Define column-name constants for player/game ids, season, week, team, position, home/away flags, and box score stats (e.g., passing yards, rushing yards, receptions), plus any mappings from nflverse-style raw names to internal standard names.
- Optionally include model/training constants (train/val/test season ranges, random seeds, list of offensive positions to keep).

## 2. Data loading layer (`data_loading.py`)

- **Implement loaders** in [`src/data_loading.py`](src/data_loading.py):
- `load_player_game_stats(path: str) -> pd.DataFrame`:
- Read Parquet via `pd.read_parquet(path)`.
- Normalize column names using mappings from `config.py` (e.g., ensure `game_id`, `player_id`, `season`, `week`, `team`, `position` are present with canonical names).
- Filter to regular-season games only (e.g., via a `season_type` / `game_type` field or equivalent mapping) and seasons `>= 2005`.
- `load_games(path: str) -> pd.DataFrame`:
- Read Parquet via `pd.read_parquet(path)`.
- Normalize to canonical `game_id`, `season`, `week`, home/away teams, and scores.
- Filter to regular season and 2005+.
- **Join contract**:
- Ensure both frames share a consistent `game_id` key and compatible season/week types for later joins and filters.

## 3. Fantasy scoring (`fantasy_scoring.py`)

- **Implement DK offense scoring** in [`src/fantasy_scoring.py`](src/fantasy_scoring.py):
- Add `compute_dk_points_offense(df: pd.DataFrame) -> pd.DataFrame`:
- Using canonical stat columns (e.g., `pass_yards`, `pass_tds`, `interceptions`, `rush_yards`, `rush_tds`, `rec_yards`, `rec_tds`, `receptions`), compute offensive DK fantasy points per the prompt.
- Implement yardage bonuses (300+ pass yards, 100+ rush yards, 100+ rec yards) with boolean masks.
- Create a new `dk_points` column and return the augmented dataframe.
- Use mappings in `config.py` to translate raw nflverse stat names to these canonical names before scoring.

## 4. Player-level z-scores & processed dataset (`feature_engineering.py`)

- **Implement z-score computation** in [`src/feature_engineering.py`](src/feature_engineering.py):
- `add_player_dk_stats(df: pd.DataFrame, min_games: int) -> pd.DataFrame`:
- Assume `df` already has columns: `player_id`, `dk_points`, `season`, `week`, `game_id`, `team`, `opponent`, `position`.
- Filter to offensive positions (and optionally kicker) using a list from `config.py`.
- Group by `player_id` (optionally `position` as a secondary key if needed) to compute `mu_player` and `sigma_player` over all games.
- Drop players with fewer than `min_games` appearances.
- Replace any `sigma_player == 0` with a small epsilon (e.g., `0.1`) to prevent division by zero.
- Compute `z = (dk_points - mu_player) / sigma_player` and add `mu_player`, `sigma_player`, and `z` columns.
- **Persist processed data**:
- After z-score augmentation, write to `data/nfl_processed/player_games_with_z.parquet` for reuse.

## 5. Pairwise training dataset construction (`build_pairwise_dataset.py`)

- **Design pairwise builder** in [`src/build_pairwise_dataset.py`](src/build_pairwise_dataset.py):
- Implement `build_pairwise_dataset(player_games_df: pd.DataFrame, games_df: pd.DataFrame) -> pd.DataFrame`:
- Merge `player_games_df` with `games_df` on `game_id` to attach game-level metadata (scores, home/away flags, etc.).
- Within each game, filter to offensive players only.
- For each game `g`, generate all unordered player pairs `(i, j)` with `i < j`:
- Create `A_` and `B_` versions of key player features: position, team, home/away indicator, `mu_player`, `sigma_player`, basic per-season-to-date averages.
- Build season-to-date rolling features using prior games only (e.g., sort by `season`, `week`, then use `groupby(player_id).expanding().mean().shift(1)` or similar to avoid leakage).
- Compute simple usage proxies such as share of team season-to-date `dk_points` or yards; if this is complex, start with season-to-date average `dk_points` and key yardage stats.
- For each pair, define target `y = z_i * z_j` and include `A_player_id`, `B_player_id`.
- Add game-level features (season, week, total points, point differential, and optionally divisional flag if derivable from teams).
- Return a model-ready dataframe with numeric and categorical columns clearly separated or flagged.

## 6. Correlation regression model (`train_corr_model.py`)

- **Prepare model training** in [`src/train_corr_model.py`](src/train_corr_model.py):
- Implement `train_corr_regressor(pairwise_df: pd.DataFrame)`:
- Define the target `y` as the `z_A * z_B` column.
- Identify categorical features (positions, teams, home/away flags) vs numerical features (means, stds, rolling stats, game totals).
- Build a `sklearn` `ColumnTransformer` to one-hot encode categorical features and pass numerical features through.
- Split data into train/val/test by season ranges (e.g., 2005–2017 train, 2018–2020 validation, 2021+ test) using a config constant.
- Choose `HistGradientBoostingRegressor` (with fallback to `RandomForestRegressor` if needed) wrapped in a `Pipeline` that combines preprocessing and model.
- Train on the train set, evaluate on validation and test sets using RMSE/MAE, and optionally print metrics.
- Save the fitted pipeline (preprocessing + model) to `models/corr_model.pkl` using `joblib.dump`.
- Ensure later inference can clamp predictions to `[-1, 1]` (either via a helper function or wrapping logic in downstream usage).

## 7. Sabersim projections to correlation matrix (`build_corr_matrix_from_projections.py`)

- **Sabersim loader and feature builder** in [`src/build_corr_matrix_from_projections.py`](src/build_corr_matrix_from_projections.py):
- Implement a loader for Sabersim CSV:
- Read `config.SABERSIM_CSV` via `pd.read_csv`.
- Filter out Captain rows by detecting duplicated player names with higher salary for CPT and keeping only FLEX entries.
- Normalize column names to internal canonical forms (e.g., `pass_yards_proj`, `rush_yards_proj`, `rec_yards_proj`, `dk_proj`).
- Implement `build_corr_matrix_from_sabersim(model, sabersim_df: pd.DataFrame) -> pd.DataFrame`:
- Derive team-level proxies (e.g., sum of projected DK points by team; optionally convert to game-level totals if needed).
- For all unordered player pairs `(A, B)` in the Sabersim FLEX set, construct feature rows mirroring the training schema:
- Player-level features: position, team, home/away if recoverable (e.g., based on matchup info in filename or extra columns), projected DK points, projected stats, usage proxies (share of team projected DK points or yards).
- Game-level features: approximate team totals from projections and any simple derivatives (e.g., point differential proxy if Vegas spread is unavailable).
- Use the saved model pipeline to predict `y_hat` for each pair and clamp to `[-1, 1]`.
- Assemble a symmetric square matrix with players as index/columns, fill diagonal as `1.0`, and ensure strict symmetry (`corr[i, j] == corr[j, i]`).

## 8. Main script and Excel output (`main.py`)

- **Orchestrate pipeline** in [`src/main.py`](src/main.py):
- Implement a main function / CLI that performs the following steps:

1. Check whether `models/corr_model.pkl` exists; if not, run the historical pipeline:

- Load raw player-game stats and games using `data_loading` functions.
- Compute DK offensive points using `compute_dk_points_offense`.
- Add player means, stds, and z-scores via `add_player_dk_stats` and persist the processed parquet.
- Build the pairwise dataset via `build_pairwise_dataset`.
- Train and save the correlation regressor via `train_corr_regressor`.

1. Load the trained model pipeline from `models/corr_model.pkl`.
2. Load Sabersim projections from `config.SABERSIM_CSV` and filter to FLEX rows.
3. Build the player correlation matrix via `build_corr_matrix_from_sabersim`.
4. Use `pd.ExcelWriter(config.OUTPUT_CORR_EXCEL, engine="xlsxwriter")` to write:

- Sheet `"Sabersim_Projections"`: FLEX-only, lightly cleaned Sabersim dataframe.
- Sheet `"Correlation_Matrix"`: correlation matrix with player names as index and columns.
- Optionally expose CLI arguments (e.g., `--retrain`, `--sabersim-path`, `--output-path`) using `argparse`, defaulting to values in `config.py`.

## 9. Documentation and README

- **Add README** ([`README.md`](README.md)):
- Briefly describe the purpose of the project and the correlation modeling approach.
- Document required dependencies (Python 3.10+, `pandas`, `numpy`, `scikit-learn`, `joblib`, `xlsxwriter`, etc.) and how to install them (e.g., via `requirements.txt`).
- Provide example commands to:
- Place nflverse Parquet files in `data/nfl_raw/`.
- Update `src/config.py` paths if needed.
- Run `python -m src.main` (or `python src/main.py`) to train the model (on first run) and generate `outputs/correlations/showdown_corr_matrix.xlsx`.

## 10. Quality and extensibility considerations

- **Code quality**:
- Add type hints and docstrings to every public function; keep modules focused by responsibility.
- Handle missing/NaN values in key fields sensibly (e.g., fill zeros for missing yardage stats where appropriate, drop rows when IDs or essential fields are missing).
- **Extensibility**:
- Centralize any nflverse- or Sabersim-specific schema assumptions in `config.py` or small helper functions so that future schema changes can be adjusted with minimal code edits.
- Design functions to be re-usable in later stages (e.g., correlation matrix builder can be reused for different slates by changing `SABERSIM_CSV`).

### To-dos

- [x] Create project directories and add config module with paths and column mappings.
- [x] Implement Parquet loading and filtering for player-game and games data with canonical columns.
- [x] Implement DraftKings offensive fantasy point computation and integrate with loaded player-game stats.
- [x] Implement per-player DK stats, z-score calculation, offensive player filtering, and save processed parquet.
- [x] Build pairwise training dataset with player-level and game-level features and target y=z_A*z_B.
- [ ] Train correlation regression model with time-based splits and save sklearn pipeline to models/corr_model.pkl.
- [ ] Load Sabersim projections, build pairwise features, apply model, and assemble correlation matrix.
- [ ] Implement main orchestration script to run full pipeline if needed and write Excel output.
- [ ] Write README with setup instructions, dependencies, and how to run the pipeline.